[
["index.html", "Data Science With Python Chapter 1 Overview", " Data Science With Python L A Liggett 2019-12-11 Chapter 1 Overview These are some notes that may be helpful for computational biology analysis that focuses on Python use. "],
["python.html", "Chapter 2 Python 2.1 General 2.2 Numba 2.3 Cython", " Chapter 2 Python 2.1 General Format strings are new in python 3 and are a different manner of using variables within a string. name = &#39;Torpal&#39; print(f&quot;hello, {name}!&quot;) Values can also be passed to the string in a manner similar to python 2. name = &#39;Torpal&#39; print(&quot;Hello {}&quot;.format(name)) Here is a general skeleton that can be used to start a python script that takes input. #!/usr/bin/env python def runArgparse(): import argparse parser = argparse.ArgumentParser() parser.add_argument(&#39;--indir&#39;, &#39;-i&#39;, type=str, nargs=&#39;*&#39;, help=&#39;Input directory containing the vcf files to be analyzed: /dir.&#39;) parser.add_argument(&#39;--loadolddata&#39;, &#39;-o&#39;, action=&#39;store_true&#39;, help=&#39;Load previously existing data.&#39;) args = parser.parse_args() indir = args.indir return indir if __name__ == &#39;__main__&#39;: runArgparse() 2.2 Numba Numba speeds up python code without having to switch to a different interpreter, and doesn’t require static typing of variables as Cython does. Just calling Numba will increase the speed of a script (except during the compilation which will add some time). But this isn’t the best way to take advantage of the speed boost. Here is an example script that uses jit to invoke Numba. #!/usr/bin/env python from numba import jit import numpy as np import time def go_slow(x): for i in range(14): x *= x @jit(nopython=True) def go_fast(x): for i in range(14): x *= x # DO NOT REPORT THIS... COMPILATION TIME IS INCLUDED IN THE EXECUTION TIME! start = time.time() go_slow(5) end = time.time() print(&quot;Elapsed slow (with compilation) = %s&quot; % (end - start)) start = time.time() go_fast(5) end = time.time() print(&quot;Elapsed fast (with compilation) = %s&quot; % (end - start)) # DO NOT REPORT THIS... COMPILATION TIME IS INCLUDED IN THE EXECUTION TIME! start = time.time() go_slow(5) end = time.time() print(&quot;Elapsed slow (after compilation) = %s&quot; % (end - start)) start = time.time() go_fast(5) end = time.time() print(&quot;Elapsed fast (after compilation) = %s&quot; % (end - start)) Parallelization can be used to automatically utilize multiple cores. from numba import njit, prange @njit(parallel=True) def prange_test(A): s = 0 for i in prange(A.shape[0]): s += A[i] return s 2.3 Cython Installation conda install -c conda-forge cython Ipython usage %load_ext Cython %%cython def f(x): return 2 * x or def f(int x): return 2 * x timeit(f(4)) "],
["jupyter.html", "Chapter 3 JupyterLab", " Chapter 3 JupyterLab By default, notebooks are not trusted, to prevent them from executing malicious code. The following code will label a notebook as trustworthy. jupyter trust my_notebook.ipyb Here is a simple template that I use that controls a couple useful things when starting a new notebook. import sys sys.path.append(&#39;../util&#39;) %reload_ext autoreload %autoreload 2 from util import * import numpy as np import pandas as pd from matplotlib import pyplot as plt import seaborn as sns sns.set_palette(&#39;pastel&#39;) sns.set_style(&#39;ticks&#39;) sns.set_context(&#39;paper&#39;, font_scale=1) It is often convenient to have a notebook automatically refresh the imported libraries so that they can be modified while working on a JupyterLab notebook. %reload_ext autoreload %autoreload 2 To allow directory organization, dependcies can be separated into different directories and imported into a jupyter notebook using the following import statement. import sys sys.path.append(&#39;../util&#39;) This is a simplified way to create a table of contents. # Table of Contents 1. [Introduction](#Notebook-Setup) 2. [Parameters](#Parameters) 3. [Sample Processing](#Sample-Processing) 1. [Variant Filtering](#Variant-Filtering) A table of contents can be created to refer to each of the headers throughout a notebook in html format. The code is below (Obviously needs to be simplified.) &lt;h1&gt;Table of Contents&lt;span class=&quot;tocSkip&quot;&gt;&lt;/span&gt;&lt;/h1&gt; &lt;div class=&quot;toc&quot;&gt; &lt;ul class=&quot;toc-item&quot;&gt; &lt;li&gt; &lt;span&gt;&lt;a href=&quot;#Python-Setup&quot; data-toc-modified-id=&quot;Python-Setup-1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Python Setup&lt;/a&gt;&lt;/span&gt; &lt;ul class=&quot;toc-item&quot;&gt; &lt;li&gt; &lt;span&gt;&lt;a href=&quot;#Change-the-width-of-the-page&quot; data-toc-modified-id=&quot;Change-the-width-of-the-page-1.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Change the width of the page&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;&lt;a href=&quot;#Import-packages&quot; data-toc-modified-id=&quot;Import-packages-1.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;Import packages&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;span&gt;&lt;a href=&quot;#Colours&quot; data-toc-modified-id=&quot;Colours-2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2&amp;nbsp;&amp;nbsp;&lt;/span&gt;Colours&lt;/a&gt;&lt;/span&gt; &lt;ul class=&quot;toc-item&quot;&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Colour-line-graph&quot; data-toc-modified-id=&quot;Colour-line-graph-2.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Colour line graph&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Totals-for-studies&quot; data-toc-modified-id=&quot;Totals-for-studies-3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3&amp;nbsp;&amp;nbsp;&lt;/span&gt;Totals for studies&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Functions-for-calculating-trinucleotide-context-specific-mutation-rates&quot; data-toc-modified-id=&quot;Functions-for-calculating-trinucleotide-context-specific-mutation-rates-4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4&amp;nbsp;&amp;nbsp;&lt;/span&gt;Functions for calculating trinucleotide-context specific mutation rates&lt;/a&gt;&lt;/span&gt; &lt;ul class=&quot;toc-item&quot;&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Calculating-mutation-rates-for-individual-variants&quot; data-toc-modified-id=&quot;Calculating-mutation-rates-for-individual-variants-4.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Calculating mutation rates for individual variants&lt;/a&gt;&lt;/span&gt; &lt;ul class=&quot;toc-item&quot;&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#DNMT3A&quot; data-toc-modified-id=&quot;DNMT3A-4.1.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.1.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;DNMT3A&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TET2&quot; data-toc-modified-id=&quot;TET2-4.1.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.1.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;TET2&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#ASXL1&quot; data-toc-modified-id=&quot;ASXL1-4.1.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.1.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;ASXL1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TP53&quot; data-toc-modified-id=&quot;TP53-4.1.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.1.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;TP53&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Calculating-mutation-rates-from-a-.csv-file-of-variants&quot; data-toc-modified-id=&quot;Calculating-mutation-rates-from-a-.csv-file-of-variants-4.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;Calculating mutation rates from a .csv file of variants&lt;/a&gt;&lt;/span&gt; &lt;ul class=&quot;toc-item&quot;&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#DNMT3A&quot; data-toc-modified-id=&quot;DNMT3A-4.2.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.2.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;DNMT3A&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TET2&quot; data-toc-modified-id=&quot;TET2-4.2.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.2.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;TET2&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#ASXL1&quot; data-toc-modified-id=&quot;ASXL1-4.2.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.2.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;ASXL1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TP53&quot; data-toc-modified-id=&quot;TP53-4.2.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.2.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;TP53&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Calculating-mutation-rates-from-a-list-of-variants&quot; data-toc-modified-id=&quot;Calculating-mutation-rates-from-a-list-of-variants-4.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;Calculating mutation rates from a list of variants&lt;/a&gt;&lt;/span&gt; &lt;ul class=&quot;toc-item&quot;&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#DNMT3A&quot; data-toc-modified-id=&quot;DNMT3A-4.3.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.3.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;DNMT3A&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TET2&quot; data-toc-modified-id=&quot;TET2-4.3.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.3.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;TET2&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#ASXL1&quot; data-toc-modified-id=&quot;ASXL1-4.3.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.3.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;ASXL1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TP53&quot; data-toc-modified-id=&quot;TP53-4.3.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.3.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;TP53&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Lists-of-variants-targeted-by-each-study&quot; data-toc-modified-id=&quot;Lists-of-variants-targeted-by-each-study-5&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5&amp;nbsp;&amp;nbsp;&lt;/span&gt;Lists of variants targeted by each study&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Jaiswal-2014&quot; data-toc-modified-id=&quot;Jaiswal-2014-5.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Jaiswal 2014&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Genovese-2014&quot; data-toc-modified-id=&quot;Genovese-2014-5.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;Genovese 2014&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#McKerrel-2015&quot; data-toc-modified-id=&quot;McKerrel-2015-5.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;McKerrel 2015&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Zink-2017&quot; data-toc-modified-id=&quot;Zink-2017-5.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;Zink 2017&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Coombs-2017&quot; data-toc-modified-id=&quot;Coombs-2017-5.5&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5.5&amp;nbsp;&amp;nbsp;&lt;/span&gt;Coombs 2017&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Young-2016-&amp;amp;-2019&quot; data-toc-modified-id=&quot;Young-2016-&amp;amp;-2019-5.6&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5.6&amp;nbsp;&amp;nbsp;&lt;/span&gt;Young 2016 &amp;amp; 2019&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Desai-2018&quot; data-toc-modified-id=&quot;Desai-2018-5.7&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5.7&amp;nbsp;&amp;nbsp;&lt;/span&gt;Desai 2018&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Acuna-Hidalgo-2017&quot; data-toc-modified-id=&quot;Acuna-Hidalgo-2017-5.8&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5.8&amp;nbsp;&amp;nbsp;&lt;/span&gt;Acuna-Hidalgo 2017&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Lists-of-all-possible-variants-in-DNMT3A,-TET2,-ASXL1,-TP53&quot; data-toc-modified-id=&quot;Lists-of-all-possible-variants-in-DNMT3A,-TET2,-ASXL1,-TP53-6&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;6&amp;nbsp;&amp;nbsp;&lt;/span&gt;Lists of all possible variants in DNMT3A, TET2, ASXL1, TP53&lt;/a&gt;&lt;/span&gt; &lt;ul class=&quot;toc-item&quot;&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#DNMT3A&quot; data-toc-modified-id=&quot;DNMT3A-6.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;6.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;DNMT3A&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TET2&quot; data-toc-modified-id=&quot;TET2-6.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;6.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;TET2&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#ASXL1&quot; data-toc-modified-id=&quot;ASXL1-6.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;6.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;ASXL1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TP53&quot; data-toc-modified-id=&quot;TP53-6.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;6.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;TP53&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Actual-number-of-observations-of-each-variant&quot; data-toc-modified-id=&quot;Actual-number-of-observations-of-each-variant-7&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;7&amp;nbsp;&amp;nbsp;&lt;/span&gt;Actual number of observations of each variant&lt;/a&gt;&lt;/span&gt; &lt;ul class=&quot;toc-item&quot;&gt; &lt;li&gt; &lt;ul class=&quot;toc-item&quot;&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#DNMT3A&quot; data-toc-modified-id=&quot;DNMT3A-7.0.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;7.0.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;DNMT3A&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TET2&quot; data-toc-modified-id=&quot;TET2-7.0.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;7.0.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;TET2&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#ASXL1&quot; data-toc-modified-id=&quot;ASXL1-7.0.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;7.0.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;ASXL1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TP53&quot; data-toc-modified-id=&quot;TP53-7.0.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;7.0.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;TP53&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Functions-for-calculating-the-expected-number-of-observations-of-a-variant&quot; data-toc-modified-id=&quot;Functions-for-calculating-the-expected-number-of-observations-of-a-variant-8&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;8&amp;nbsp;&amp;nbsp;&lt;/span&gt;Functions for calculating the expected number of observations of a variant&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Maximum-Likelihood-Estimation-for-s&quot; data-toc-modified-id=&quot;Maximum-Likelihood-Estimation-for-s-9&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;9&amp;nbsp;&amp;nbsp;&lt;/span&gt;Maximum Likelihood Estimation for s&lt;/a&gt;&lt;/span&gt; &lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#DNMT3A-variants&quot; data-toc-modified-id=&quot;DNMT3A-variants-9.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;9.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;DNMT3A variants&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TET2-variants&quot; data-toc-modified-id=&quot;TET2-variants-9.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;9.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;TET2 variants&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#ASXL1-variants&quot; data-toc-modified-id=&quot;ASXL1-variants-9.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;9.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;ASXL1 variants&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;&lt;a href=&quot;#TP53-variants&quot; data-toc-modified-id=&quot;TP53-variants-9.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;9.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;TP53 variants&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; "],
["visualization.html", "Chapter 4 Visualization 4.1 Color 4.2 Matplotlib 4.3 Seaborn 4.4 Statistics 4.5 Various Plot Styles", " Chapter 4 Visualization 4.1 Color 4.1.1 Colorschemes Seaborn Themes Pastel: {&#39;Blue&#39;:&#39;#a3c6ff&#39;, &#39;Orange&#39;:&#39;#f7ab60&#39;, &#39;Green&#39;:&#39;#60f7a9&#39;, &#39;Red&#39;:&#39;#fc9d94&#39;, &#39;Purple&#39;:&#39;#bea3ff&#39;, &#39;Brown&#39;:&#39;#d1b485&#39;, &#39;Pink&#39;:&#39;#f7afdf&#39;, &#39;Gray&#39;:&#39;#c4c4c4&#39;, &#39;Yellow&#39;:&#39;#ffffaa&#39;, &#39;LBlue&#39;:&#39;#baf6ff&#39;} Deep: {&#39;Green&#39;:&#39;#5baf68&#39;} 4.1.2 Controlling Coloration Not all plots automatically plot with a white background, and when using something dark like jupyterlab or a presentation this can be frustrating. The background color can be set in pyplot like this. fig.patch.set_facecolor(&#39;xkcd:mint green&#39;) When plotting, samples will not always be colored with the same color, especially when different subsets of samples are included in different plots. Here is a manual workaround to specify the coloration of displayed data. This is a bit cumbersome so there might be a more elegant way of achieving the same outcome. # here is an example where sample order is controlled from a pandas DataFrame sample_order = all_vars.sort_values([&#39;ID&#39;]).drop_duplicates([&#39;Sample&#39;]).Sample # the color order is specified here # colors should be in the same order as the above sample_order Series, excluding samples with no data colors = [pastel[&#39;Brown&#39;], pastel[&#39;Blue&#39;], pastel[&#39;Orange&#39;], pastel[&#39;Purple&#39;], pastel[&#39;Green&#39;], pastel[&#39;Red&#39;], ] plt.figure() # this is an example of plotting that uses the sample_order and palette to control coloration order sns.catplot(x=&#39;Sample&#39;, y=&#39;VAF&#39;, hue=&#39;Gene&#39;, jitter=True, data=oncogenic[oncogenic.Location == &#39;Peripheral&#39;], legend=False, order=sample_order, palette=sns.color_palette(colors)) # a colorscheme can be specified if desired pastel = {&#39;Blue&#39;:&#39;#a3c6ff&#39;, &#39;Orange&#39;:&#39;#f7ab60&#39;, &#39;Green&#39;:&#39;#60f7a9&#39;, &#39;Red&#39;:&#39;#fc9d94&#39;, &#39;Purple&#39;:&#39;#bea3ff&#39;, &#39;Brown&#39;:&#39;#d1b485&#39;, &#39;Pink&#39;:&#39;#f7afdf&#39;, &#39;Gray&#39;:&#39;#c4c4c4&#39;, &#39;Yellow&#39;:&#39;#ffffaa&#39;, &#39;LBlue&#39;:&#39;#baf6ff&#39;} # this controls the coloration in the legend import matplotlib.patches as mpatches egfr = mpatches.Patch(color=pastel[&#39;Blue&#39;], label=&#39;EGFR&#39;) pik3ca = mpatches.Patch(color=pastel[&#39;Orange&#39;], label=&#39;PIK3CA&#39;) myc = mpatches.Patch(color=pastel[&#39;Green&#39;], label=&#39;MYC&#39;) plt.legend(handles=[egfr,pik3ca,myc], loc=&#39;upper right&#39;, bbox_to_anchor=(1.5, 1), ncol=1) # no legend overlap 4.2 Matplotlib Plot a straight line where num_points is the number of elements to populate the array with (default is 50). x = np.linspace(x_min, x_max, num_points) y = m * x + b plt.plot(x, y, &#39;-b&#39;) Plotting a heatmap. import matplotlib.pyplot as plt import numpy as np a = np.random.random((16, 16)) plt.imshow(a, cmap=&#39;RdBu&#39;&#39;, interpolation=&#39;nearest&#39;) plt.show() Possible heatmap colors are: Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples, Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, Set3_r, Spectral, Spectral_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r, afmhot, afmhot_r, autumn, autumn_r, binary, binary_r, bone, bone_r, brg, brg_r, bwr, bwr_r, cividis, cividis_r, cool, cool_r, coolwarm, coolwarm_r, copper, copper_r, cubehelix, cubehelix_r, flag, flag_r, gist_earth, gist_earth_r, gist_gray, gist_gray_r, gist_heat, gist_heat_r, gist_ncar, gist_ncar_r, gist_rainbow, gist_rainbow_r, gist_stern, gist_stern_r, gist_yarg, gist_yarg_r, gnuplot, gnuplot2, gnuplot2_r, gnuplot_r, gray, gray_r, hot, hot_r, hsv, hsv_r, icefire, icefire_r, inferno, inferno_r, jet, jet_r, magma, magma_r, mako, mako_r, nipy_spectral, nipy_spectral_r, ocean, ocean_r, pink, pink_r, plasma, plasma_r, prism, prism_r, rainbow, rainbow_r, rocket, rocket_r, seismic, seismic_r, spring, spring_r, summer, summer_r, tab10, tab10_r, tab20, tab20_r, tab20b, tab20b_r, tab20c, tab20c_r, terrain, terrain_r, twilight, twilight_r, twilight_shifted, twilight_shifted_r, viridis, viridis_r, vlag, vlag_r, winter, winter_r A simple venn diagram. from matplotlib_venn import venn2 venn2(subsets = (3, 2, 1)) A more complicated venn diagram. from matplotlib import pyplot as plt import numpy as np from matplotlib_venn import venn3, venn3_circles plt.figure(figsize=(4,4)) v = venn3(subsets=(1, 1, 1, 1, 1, 1, 1), set_labels = (&#39;A&#39;, &#39;B&#39;, &#39;C&#39;)) v.get_patch_by_id(&#39;100&#39;).set_alpha(1.0) v.get_patch_by_id(&#39;100&#39;).set_color(&#39;white&#39;) v.get_label_by_id(&#39;100&#39;).set_text(&#39;Unknown&#39;) v.get_label_by_id(&#39;A&#39;).set_text(&#39;Set &quot;A&quot;&#39;) c = venn3_circles(subsets=(1, 1, 1, 1, 1, 1, 1), linestyle=&#39;dotted&#39;) c[0].set_lw(1.0) c[0].set_ls(&#39;dotted&#39;) plt.title(&quot;Sample Venn diagram&quot;) plt.annotate(&#39;Unknown set&#39;, xy=v.get_label_by_id(&#39;100&#39;).get_position() - np.array([0, 0.05]), xytext=(-70,-70), ha=&#39;center&#39;, textcoords=&#39;offset points&#39;, bbox=dict(boxstyle=&#39;round,pad=0.5&#39;, fc=&#39;gray&#39;, alpha=0.1), arrowprops=dict(arrowstyle=&#39;-&gt;&#39;, connectionstyle=&#39;arc3,rad=0.5&#39;,color=&#39;gray&#39;)) plt.show() An upset plot is a nice alternative to a traditional venn diagram. The project is hosted here, and this is the documentation. First install the library. pip install upsetplot Here is the code to create the plot. import numpy as np arrays = [[False,False,False,False,True,True,True,True], [False,False,True,True,False,False,True,True], [False,True,False,True,False,True,False,True]] tuples = list(zip(*arrays)) def o(one=False, two=False, three=False): if three: temp = pd.merge(indels[(indels.Individual==one)], indels[(indels.Individual==two)], how=&#39;inner&#39;, on=[&#39;Loc&#39;, &#39;Var&#39;]) return len(pd.merge(temp, indels[(indels.Individual==three)], how=&#39;inner&#39;, on=[&#39;Loc&#39;, &#39;Var&#39;])) elif two: return len(pd.merge(indels[(indels.Individual==one)], indels[(indels.Individual==two)], how=&#39;inner&#39;, on=[&#39;Loc&#39;, &#39;Var&#39;])) elif one: return len(indels[(indels.Individual==one)]) else: return 0 index = pd.MultiIndex.from_tuples(tuples, names=[&#39;Ind 1&#39;, &#39;Ind 2&#39;, &#39;Ind 3&#39;]) s = pd.Series([o(), o(3), o(2), o(2,3), o(1), o(1,3), o(1,2), o(1,2,3)], index=index) from upsetplot import plot as up up(s) plt.savefig(&quot;../images/indels.svg&quot;, format=&quot;svg&quot;, bbox_inches=&quot;tight&quot;) Log scales seem to always be a challenge. Here is at least one solution to change ticks to log manually. y_major_ticks = [np.log(100),np.log(200),np.log(300),np.log(400),np.log(500),np.log(600),np.log(700),np.log(800),np.log(900),\\ np.log(1000),np.log(2000),np.log(3000),np.log(4000),np.log(5000),np.log(6000),np.log(7000),np.log(8000),np.log(9000),\\ np.log(10000),np.log(20000),np.log(30000),np.log(40000),np.log(50000),np.log(60000),np.log(70000),np.log(80000),np.log(90000),\\ np.log(100000),np.log(200000),np.log(300000),np.log(400000),np.log(500000),np.log(600000),np.log(700000),np.log(800000),np.log(900000),\\ np.log(1000000),np.log(2000000),np.log(3000000),np.log(4000000),np.log(5000000),np.log(6000000),np.log(7000000),np.log(8000000),np.log(9000000),\\ np.log(10000000)] y_major_tick_labels = [&quot;100&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;, &quot;1000&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;, &quot;10,000&quot;,\\ &quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;100,000&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;, &quot;1,000,000&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;, &quot;10,000,000&quot; ] ax1.set_yticks(y_major_ticks) ax1.set_yticklabels(y_major_tick_labels, fontsize = axisfont) ax1.yaxis.set_tick_params(width=scale, color = grey3, length = 6) 4.3 Seaborn Here is a general bar plot that includes some commonly used parameters. # fits my 22 inch monitor plt.figure(figsize=(19.17,11.98)) # order controls the display order of the samples sns.catplot(x=&quot;Sample&quot;, y=&quot;Somatic&quot;, kind=&quot;bar&quot;, data=var_counts, order=labels); # keeps x-axis labels, but eliminates the tick mark plt.tick_params(labelbottom=True, bottom=False) # trim off the x-axis sns.despine(offset=10, trim=True, bottom=True) # labels plt.title(&#39;&#39;) plt.ylabel(&#39;&#39;, fontsize=8) plt.xlabel(&#39;&#39;, fontsize=8) # manual control of xlabels labels = [&#39;Indiv_1-a&#39;,&#39;Indiv_2&#39;,&#39;Indiv_3&#39;,&#39;Indiv_1-b&#39;] # control xtick order plt.xticks(range(len(labels)), labels, rotation=45) # control the number of x-ticks plt.locator_params(axis=&#39;x&#39;, nbins=10) # legend positioning plt.legend(loc=&#39;upper right&#39;) # log scale plt.gca().set_yscale(&#39;log&#39;) # this is better if neg values are needed plt.gca().set_yscale(&#39;symlog&#39;) # fit plot to display plt.tight_layout() plt.show() # save figure with tight_layout plt.savefig(&quot;test.svg&quot;, format=&quot;svg&quot;, bbox_inches=&quot;tight&quot;, dpi=1000) Signifance information can be added by including p-values and label bars using the following code. x1, x2 = 0, 1 # columns to annotate on the plot y2, y1 = 20, 15 # placement of the line and how for down the vertical legs go plt.plot([x1,x1, x2, x2], [y1, y2, y2, y1], linewidth=1, color=&#39;k&#39;) # stats line plt.text((x1+x2)*.5, y2+2, &quot;p=0.09&quot;, ha=&#39;center&#39;, va=&#39;bottom&#39;, fontsize=8) # p-value or sig Sometimes it is helpful to get the number of rows associated with a sample and plot that by sample. Here is a way to accomplish this. counts = all_vars.Sample_Name.value_counts().reset_index() counts = counts.rename(columns={&#39;index&#39;:&#39;Sample_Name&#39;,&#39;Sample_Name&#39;:&#39;Var_Count&#39;}) all_vars = pd.merge(all_vars, counts, how=&#39;inner&#39;, on=[&#39;Sample_Name&#39;]) 4.4 Statistics This is a two-sided T-test for the null hypothesis that two populations have the same means. It is important to note that it assumes the population variances are the same, so this must be changed if the assumption is incorrect. # ttest_ind(a, b, axis=0, equal_var=True, nan_policy=&#39;propagate&#39;) from scipy.stats import ttest_ind ttest_ind(df[df[&#39;sample&#39;] == &#39;one&#39;][&#39;means&#39;], df[df[&#39;sample&#39;] == &#39;two&#39;][&#39;means&#39;]) 4.5 Various Plot Styles This displays each individual datapoint overlayed on a boxplot ax = sns.boxplot(x=&#39;day&#39;, y=&#39;total_bill&#39;, data=tips) ax = sns.swarmplot(x=&#39;day&#39;, y=&#39;total_bill&#39;, data=tips, color=&#39;.25&#39;) 4.5.1 Countplot: Multiple Samples / Error Bars The typical seaborn countplot appears to only perform counts for each sample or a total from multiple samples, and therefore will not group by sample and compute error over the samples. The following code will group samples by a particular column, then calculate counts for the data in another column, and then plot the counts with error bars, colored by another column. # only retain somatic substitutions substitutions = filtered[filtered.Change.str.len() == 3] somatic = substitutions[substitutions.Group == &#39;Somatic&#39;] # group mutations together by change somatic = pd.DataFrame(somatic.groupby(&#39;Sample&#39;).Change.value_counts()) somatic.index = somatic.index.set_names([&#39;Sample&#39;,&#39;Mut&#39;]) somatic.reset_index(inplace=True) somatic = somatic.rename(columns={&#39;Mut&#39;:&#39;Change&#39;, &#39;Change&#39;:&#39;Count&#39;}) # label data as clonal or typical key = substitutions[[&#39;Sample&#39;,&#39;Clonality&#39;]].drop_duplicates(&#39;Sample&#39;) somatic = pd.merge(somatic, key, how=&#39;left&#39;, on=[&#39;Sample&#39;]) plt.figure() sns.barplot(x=&#39;Change&#39;, y=&#39;Count&#39;, hue=&#39;Clonality&#39;, data=somatic) plt.xticks(rotation=90) pretty() "],
["biology.html", "Chapter 5 Biology 5.1 General 5.2 Biopython 5.3 UCSC Genome Browser 5.4 Ref Genome 5.5 Personal Information 5.6 GATK 5.7 Samtools", " Chapter 5 Biology 5.1 General Some helpful commands for genetic sequence. from string import ascii_uppercase # python 3 from string import upper, lower # python 2 upper(&#39;tcga&#39;) lower(&#39;TCGA&#39;) title(&#39;tcga&#39;) # capitalize the first letter 5.2 Biopython Reverse complement of sequence from Bio.Seq import Seq str(Seq(i).reverse_complement()) 5.3 UCSC Genome Browser Get sequence from UCSC genome browser from subprocess import check_output, STDOUT temp = check_output(&#39;wget -qO- http://genome.ucsc.edu/cgi-bin/das/hg19/dna?segment=%s:%s,%s&#39; % (vcfObj.chrom,low,high), stderr=STDOUT, shell=True) If the reference genome comes in the .2bit format, it is likely that it should then be converted to .fa format, and .2bittofa can accomplish this. twoBitToFa refgenomes/hg19.2bit refgenomes/hg19.fa The newly created .fa file will need to be indexed if it is to be used with gatk and bwa mem. Here is an example of indexing where -a bwtsw specifies that we want to use the indexing algorithm that is capable of handling the whole human genome. bwa index -a bwtsw reference.fa Then create a file called reference.fa.fai, with one record per line for each of the contigs in the FASTA reference file. Each record is composed of the contig name, size, location, basesPerLine and bytesPerLine. samtools faidx reference.fa 5.4 Ref Genome Get sequence from reference genome from subprocess import check_output, STDOUT temp = check_output(&#39;samtools faidx %s %s:%s-%s&#39; % (ref, vcfObj.chrom, low, high), stderr=STDOUT, shell=True) finalSeq = &#39;&#39; for line in temp.decode(&#39;UTF-8&#39;).split(&#39;\\n&#39;): for line in temp.decode(&#39;UTF-8&#39;).split(&#39;\\n&#39;): # this is only necessary in python 3 to convert binary to string if &#39;&gt;&#39; not in line: finalSeq += line finalSeq = finalSeq.upper() 5.5 Personal Information # parse vcf file with parseline if &#39;#&#39; not in line and &#39;chr&#39; in line: # skip the info # vcf handling from parseline import VCFObj # or from util import VCFObj vcfObj = VCFObj(vcfLine) # available attributes: ao, dp, af, wt, var, chrom, location 5.6 GATK The entire set of GATK software can be downloaded together here. Methods and algorithms as they are run at the Broad can be found here. The Broad also has a number of tutorials that introduce some of the software packaged within GATK. 5.6.1 Mutect2 A tutorial exists that introduces how Mutect2 can be used to call somatic mutations. In general Mutect2 can be run as in the following. ../util/gatk-4.1.4.0/gatk Mutect2 -R /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta -I tumor.bam -O unfiltered.vcf 5.7 Samtools Grab a single chromosomal region from a .bam file. samtools view -b in.bam chr1 &gt; in_chr1.bam "],
["io.html", "Chapter 6 Data I/O 6.1 Reading Data Files 6.2 Pickles", " Chapter 6 Data I/O 6.1 Reading Data Files Opening .gz files import gzip for line in gzip.open(&#39;myFile.gz&#39;): print line 6.2 Pickles Writing data in pickle format import pickle p = open(&#39;principle.pkl&#39;, &#39;wb&#39;) pickle.dump(principleData, p) p.close() Reading data in pickle format import pickle p = open(&#39;principle.pkl&#39;, &#39;rb&#39;) principleData = pickle.load(p) p.close() "],
["pandas.html", "Chapter 7 Pandas 7.1 File I/O 7.2 Data Structure Creation 7.3 Selection 7.4 Splitting 7.5 Relabeling 7.6 Sorting and Arranging 7.7 Editing Data 7.8 Combining Data Structures 7.9 Summarizing 7.10 Arithmetic and Row/Column-wise Analysis 7.11 Restructuring DataFrames", " Chapter 7 Pandas 7.1 File I/O Read a csv file into a DataFrame. pd.read_csv(filepath) Write a DataFrame to a file. x.to_csv(path_or_buf=&#39;outputDir&#39;, sep=&#39;\\n&#39;, header=False, index=False) 7.2 Data Structure Creation Create a DataFrame. frame = pd.DataFrame(np.random.randn(4,3), columns=list(&#39;bde&#39;), index=[&#39;Utah&#39;,&#39;Ohio&#39;,&#39;Texas&#39;,&#39;Oregon&#39;]) A DataFrame can conveniently be created from a dictionary. import pandas as pd data = {&#39;AAA&#39; : [4,5,6,7], &#39;BBB&#39; : [10,20,30,40],&#39;CCC&#39; : [100,50,-30,-50]} df2 = pd.DataFrame(data=data,index=[1,2,3,4]) #Note index starts at 1. df2 ## AAA BBB CCC ## 1 4 10 100 ## 2 5 20 50 ## 3 6 30 -30 ## 4 7 40 -50 7.3 Selection Is data within a DataFrame found within a dictionary or list? (Instead of a dictionary a series can be used and maybe another DataFrame) import pandas as pd df = pd.DataFrame({&#39;A&#39;: [1, 2, 3], &#39;B&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;f&#39;]}) df.isin([1, 3, 12, &#39;a&#39;]) ## A B ## 0 True True ## 1 False False ## 2 True False df[df.isin([1, 3, 12, &#39;a&#39;])] ## A B ## 0 1.0 a ## 1 NaN NaN ## 2 3.0 NaN Data within a DataFrame can be selected based on position within the DataFrame. import pandas as pd df2.iloc[1:3] ## AAA BBB CCC ## 2 5 20 50 ## 3 6 30 -30 Select data by the length of the strings in a given column. df = df[df.Change.str.len() == 3] Data within a DataFrame can be selected based on position within the DataFrame. import pandas as pd df2.loc[1:3] ## AAA BBB CCC ## 1 4 10 100 ## 2 5 20 50 ## 3 6 30 -30 The opposite of matching data can be selected with the inverse operator. df[~((df.AAA &lt;= 6) &amp; (df.index.isin([0,2,4])))] 7.4 Splitting Concatenate two DataFrames together without dropping any values or renaming indices. left = pd.concat([left,left]) Concatenate two DataFrames together without dropping values, but renaming index. left = pd.concat([left,left], ignore_index=True) Count the number of each unique value in a specified column. left[&#39;key1&#39;].value_counts() left.key1.value_counts() Value counts can also be calculated as percentages so that raw counts as percent makeup can be compared. left[&#39;key1&#39;].value_counts(normalize=True) * 100 Two DataFrames can be merged such that only the data containing matching keys is retained. result = pd.merge(left, right, how=&#39;inner&#39;, on=[&#39;key1&#39;, &#39;key2&#39;]) This DataFrame merge will retain all of the data in the right DataFrame. result = pd.merge(left, right, how=&#39;right&#39;, on=[&#39;key1&#39;, &#39;key2&#39;]) Filter by multiple columns. df[(df.one == 1) &amp; (df.two == 2)] Filter by multiple columns but only return certain values. # this just returns the data in column AAA df = pd.DataFrame({&#39;AAA&#39; : [4,5,6,7], &#39;BBB&#39; : [10,20,30,40],&#39;CCC&#39; : [100,50,-30,-50]}) newseries = df.loc[(df[&#39;BBB&#39;] &lt; 25) &amp; (df[&#39;CCC&#39;] &gt;= -40), &#39;AAA&#39;] Filtering by values and using assignment will modify the original DataFrame. df.loc[(df[&#39;BBB&#39;] &gt; 25) | (df[&#39;CCC&#39;] &gt;= 75), &#39;AAA&#39;] = 0.1 Select multiple values from a particular column, where Letter is the column header. df[df.Letter.isin([&#39;a&#39;,&#39;b&#39;])] Use itertools to find combinations of data within a column of two DataFrames. itertools.product(df1[&#39;a&#39;], df2[&#39;a&#39;]) Add data to a particular cell within a DataFrame. df.loc[index,column]=num Make a copy of a DataFrame. df.copy(deep=True) Iterate through a DataFrame. for i in df.itertuples(): pass Change order of columns. x = x.reindex(columns=[&#39;header&#39;,&#39;seq&#39;,&#39;plus&#39;,&#39;qual&#39;]) Make a DataFrame from a dictionary d = {&#39;col1&#39;: [1, 2], &#39;col2&#39;: [3, 4]} x = pd.DataFrame(d) Sample from a DataFrame. df.sample(frac=1) df.sample(n=20, axis=1) Append to a DataFrame. df=df.append(newdf, ignore_index=True) # without ignore_index, the original indices will be used Remove duplicates x = x[~x.index.duplicated(keep=&#39;first&#39;)] # most ideal method data = pd.DataFrame({&#39;k1&#39;:[&#39;one&#39;,&#39;two&#39;]*3+[&#39;two&#39;],&#39;k2&#39;:[1,1,2,3,3,4,4]}) data.duplicated() # identify duplicate data data[‘k1’].duplicated() data[&#39;k1&#39;].drop_duplicates() data.drop_duplicates[&#39;k1&#39;] # this does the same thing as the previous line data.drop_duplicates([&#39;k1&#39;,&#39;k2&#39;], keep=&#39;last&#39;) # drops unique found in k1 and k2 and keeps the last indexed duplicate Check if string is within strings in a given column x[x[&#39;strLoc&#39;].str.contains(region)] 7.5 Relabeling Rename a column or group of columns can be done by passing a dictionary of the changes. df = df.rename(columns={&#39;a&#39;:&#39;b&#39;,&#39;c&#39;:&#39;d&#39;}) 7.6 Sorting and Arranging The data in a DataFrame can be sorted in numeric or lexicographic order. The following code sorts the values within the columns a and b. df.sort_values([&#39;a&#39;,&#39;b&#39;], ascending=False) Set a column as the new index x.set_index([&#39;uniques&#39;]) 7.7 Editing Data Drop columns from a DataFrame. import numpy as np df = pd.DataFrame(np.arange(12).reshape(3,4), columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]) print(df) df = df.drop(columns=[&#39;B&#39;, &#39;C&#39;]) # may not work in python 2 df = df.drop([&#39;B&#39;, &#39;C&#39;], axis=1) # this works in python 2 print(df) Changing the datatype of a column of data can be done by just changing column type. df.Age = df.Age.astype(str) Replace values. data = pd.Series([1., -999., 2., -999., -1000., 3.]) data.replace(-999, np.nan) Substrings can be extracted from each row or column using the str functionality. Series.str.slice(start=0,stop=7,step=1) 7.7.1 Replace Values New data can be set within a DataFrame one subset at a time in a way that will avoid the SettingWithCopyWarning. import pandas as pd df = pd.DataFrame({&#39;Trait&#39;:[&#39;Seed_Shape&#39;,&#39;Seed_Shape&#39;,&#39;Flower_Color&#39;,&#39;Flower_Color&#39;], &#39;Phenotype&#39;:[&#39;Round&#39;,&#39;Wrinkled&#39;,&#39;Purple&#39;,&#39;White&#39;]}) df.loc[df.Trait == &#39;Seed_Shape&#39;, &#39;Affected_Part&#39;] = &#39;Seed&#39; df.loc[df.Trait == &#39;Flower_Color&#39;, &#39;Affected_Part&#39;] = &#39;Flower&#39; print(df) ## Trait Phenotype Affected_Part ## 0 Seed_Shape Round Seed ## 1 Seed_Shape Wrinkled Seed ## 2 Flower_Color Purple Flower ## 3 Flower_Color White Flower There is a more simple alternative to the above method buit it may result in the SettingWithCopyWarning. df = df.replace(&#39;pork&#39;,&#39;bacon&#39;) 7.8 Combining Data Structures The following merges df and df2 using inner to get the intersection on the Sample column, where indexes are ignored if the merging is performed on a column as in the following example. The other possible merging strategies are: left: use only keys from left frame, similar to a SQL left outer join; preserve key order. right: use only keys from right frame, similar to a SQL right outer join; preserve key order. outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. df = pd.merge(df, df2, how=&#39;inner&#39;, on=[&#39;Sample&#39;]) Appending to a Dataframe attaches a DataFrame after another one. df = pd.DataFrame([[1, 2], [3, 4]], columns=list(&#39;AB&#39;)) df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list(&#39;AB&#39;)) df.append(df2) 7.9 Summarizing The mean of column values can be calculated where each of the columns is grouped by the data in a specified column. temp[[&#39;Sample&#39;,&#39;VAF&#39;,&#39;Var_Count&#39;]].groupby(&#39;Sample&#39;).mean() 7.9.1 Adding Classifier Columns New columns can be added that describe the data in another column using some conditional statement. df[&#39;Size&#39;] = np.where(df[&#39;Height&#39;] &lt;= 5, &#39;Short&#39;, &#39;Tall&#39;) 7.10 Arithmetic and Row/Column-wise Analysis Sometimes it is helpful to analyze the value in a particular cell in a conditional manner depending on it’s value and then set the result of this analysis to a corresponding cell in a new column. Here is an example where the VAF of a variant is conditionally analyzed. def LOH(x): if x &gt; 0.75: return 1 - x elif x &lt;= 0.75 and x &gt; 0.25: return abs(0.5 - x) else: return 0 all_vars[&#39;LOH&#39;] = all_vars.VAF.transform(LOH) max_loh = all_vars.groupby(&#39;Sample&#39;).LOH.max().reset_index().rename(columns={&#39;LOH&#39;:&#39;Max_LOH&#39;}) all_vars = pd.merge(all_vars, max_loh, how=&#39;inner&#39;, on=[&#39;Sample&#39;]) Broadcasting arithmetic is an efficient method of calculating across an entire DataFrame. frame = pd.DataFrame(np.arange(12.).reshape((4,3)), columns=list(&#39;bde&#39;), index=[&#39;Utah&#39;,&#39;Ohio&#39;,&#39;Texas&#39;,&#39;Oregon&#39;] series = frame.iloc[0] frame - series # the subtraction function could also be used # frame.sub(series, axis=&#39;columns&#39;) Apply a function to each row or column. f = lambda x: x.max() - x.min() frame.apply(f, axis=&#39;index&#39;) Add two sets of data together, and use fill_value to avoid replacing any missing data with NaN. x = pd.DataFrame([1,2,3], columns=list(&#39;0&#39;)) y = pd.DataFrame([1,2,3], columns=list(&#39;1&#39;)) x = x.add(y, fill_value=0) Take the mean or std across specified columns and append as a new column. Below the DataFrame has columns 1-7 that will be used in computing the mean or std and this new data will be appended in a new column labeled ‘Mean’ or ‘Std’. x[&#39;Mean&#39;]=x[[1,2,3,4,5,6,7]].mean(axis=1) x[&#39;Std&#39;]=x[[1,2,3,4,5,6,7]].std(axis=1) 7.11 Restructuring DataFrames Converting between dataframes being structured to use “dummy” or “indicator” variables, and categorical variables is easy using pandas. The following converts a DataFrame to use dummy/indicator variables. df = pd.DataFrame({&#39;A&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;], &#39;B&#39;: [&#39;b&#39;, &#39;a&#39;, &#39;c&#39;], &#39;C&#39;: [1, 2, 3]}) pd.get_dummies(df, prefix=[&#39;col1&#39;, &#39;col2&#39;]) ## C col1_a col1_b col2_a col2_b col2_c ## 0 1 1 0 0 1 0 ## 1 2 0 1 1 0 0 ## 2 3 1 0 0 0 1 The following converts a DataFrame that uses dummy/indicator variables to instead use categorical variables. df = pd.DataFrame({&#39;Blue&#39;:[1,0,0],&#39;Green&#39;:[0,1,1],&#39;Red&#39;:[0,0,0]}, index=[&#39;One&#39;,&#39;Two&#39;,&#39;Three&#39;]) df.eq(1) @ df.columns ## One Blue ## Two Green ## Three Green ## dtype: object "],
["git.html", "Chapter 8 Git 8.1 Setup 8.2 Manipulating Commits 8.3 Credential Caching 8.4 Errors", " Chapter 8 Git 8.1 Setup 8.1.1 Git Setup The username and email needs to be added after git is installed. git config --global user.name &quot;me&quot; git config --global user.email &quot;me@gmail.com&quot; After this information has been set, it can be checked. git config --list 8.1.2 Repository Initiation To setup a repository, create a folder with an initial file like a README and then initiate it. git init git status 8.1.3 Mirror on Online Repository Create a repository on a repository like github, gitlab, bitbucket, or sourceforge. Then the local git repository can be synched with the online repository. git remote add origin url-of-online-repository-here git push -u origin master Of course the repository could just be setup first and then cloned. git clone url-of-online-repository-here 8.2 Manipulating Commits 8.2.1 Repository Status The commit history of a repository can be displayed in verbose form and in summarized form. git log git log --oneline 8.2.2 File Checkout To restore a previous version of a file it can be checked out by first identifying the version to be used using the log history and then restoring the desired file. git log --oneline git checkout &lt;commit number&gt; file.txt 8.2.3 Resetting a Repository To discard the effect of the previous operation on a file. git reset HEAD file.txt The previous version of the a file can then be restored. git checkout -- file.txt 8.2.4 Branching Create a new branch. git branch somenewfeature All current branches can then be listed. git branch To then use the new branch, it needs to be checked out, and after checking it out, all changes will be specific to that new branch only. git checkout somenewfeature After committing changes to the new branch, if the branch is changed back to the master branch or another branch those changes will not longer be present as they are branch specific. If the changes should be merged back to the master branch checkout the master and then merge them in. git checkout master git merge feature 8.3 Credential Caching Credential caching will allow the username and password to be stored so commits can be expedited. git config credential.helper store git push https://github.com/owner/repo.git &gt;&gt;&gt;&gt;&gt;&gt;&gt; 325964ef5965c277600913d4433186ee42658f01 Username for &#39;https://github.com&#39;: &lt;USERNAME&gt; Password for &#39;https://USERNAME@github.com&#39;: &lt;PASSWORD&gt; To configure the caching to expire: git config --global credential.helper &#39;cache --timeout 7200&#39; 8.4 Errors There is an error where objects in the repository get corrupted resulting in an error message like this: object corrupt or missing: .git/objects/93/17fdbd6436bfaf7b0eeae2c56. To fix this error remove all of the corrupted objects until there are none left. rm .git/objects/93/17fdbd6436bfaf7b0eeae2c56 There should then be plenty of remaining errors as the files are still missing, some of the errors may look like this: dangling blob 750718546640b5b14c19cbdb9958d7bcc4b1114c. At this point it should be possible to push/push the current repository. git push origin master "],
["vim.html", "Chapter 9 VIM 9.1 Formatting 9.2 Spellcheck", " Chapter 9 VIM 9.1 Formatting Automatic newlines are inserted by default; this behavior can be overidden with the following. :set wrap :set textwidth=0 wrapmargin=0 9.2 Spellcheck To setup spellchecking first setup a personal dictionary file. # make a directory for personal dictionary mkdir -p ~/.vim/spell/ Then refer to the dictionary file within VIM, and enable spellchecking. # set personal dictionary :set spellfile=~/.vim/spell/en.utf-8.add # turn spellcheck on :set spell Get spellcheck commands. :help spell Add a word to personal dictionary. zg Move to next and previous misspelled word. ]s [s Get suggestions for misspelled word. z= "],
["apis.html", "Chapter 10 Web APIs 10.1 Ensembl 10.2 UCSC Genome Browser", " Chapter 10 Web APIs 10.1 Ensembl The Ensembl Rest API has a number of different genomics tools. Here is an example where the Rest API is used to get the genomic locus and amino acid change using only the protein name and amino acid position and identities. content-type=application/json wget -q --header=&#39;Content-type:application/json&#39; &#39;https://rest.ensembl.org/map/translation/ENSMUSP00000020991/878..879?&#39; -O - Alternatively python can be used to make the same call. import requests, sys server = &quot;https://rest.ensembl.org&quot; ext = &quot;/map/translation/ENSMUSP00000020991/878...879?&quot; r = requests.get(server+ext, headers={ &quot;Content-Type&quot; : &quot;application/json&quot;}) if not r.ok: r.raise_for_status() sys.exit() decoded = r.json() print(repr(decoded)) Here are examples getting variant effect consequences of a particular mutation. wget -q --header=&#39;Content-type:application/json&#39; &#39;https://rest.ensembl.org/vep/mus_musculus/hgvs/ENSMUSP00000020991:p.Arg878His?&#39; -O - 10.2 UCSC Genome Browser Here is an example where human genomic regions are converted to the orthologous mouse regions. First the human sequence is obtained. wget -O - http://genome.ucsc.edu/cgi-bin/das/hg38/dna?segment=%s:%s,%s &gt;&gt; locs\\n&#39; % (chrom, low, high) Then the above sequence is used in UCSC BLAT to find the orthologous region within mouse. "],
["golang.html", "Chapter 11 Golang 11.1 Installation 11.2 Updating 11.3 Sample Program 11.4 Type conversion 11.5 Strings 11.6 Boolean Functions 11.7 Variables 11.8 Input 11.9 Control Structures 11.10 Data Structures 11.11 Functions 11.12 While loops 11.13 Timing a function 11.14 Pointers 11.15 Structures 11.16 Methods 11.17 Packaging 11.18 Read/Write Files 11.19 Math 11.20 Concurrency", " Chapter 11 Golang 11.1 Installation Installation of linuxbrea sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)&quot; # Add to path test -d ~/.linuxbrew &amp;&amp; eval $(~/.linuxbrew/bin/brew shellenv) test -d /home/linuxbrew/.linuxbrew &amp;&amp; eval $(/home/linuxbrew/.linuxbrew/bin/brew shellenv) test -r ~/.bash_profile &amp;&amp; echo &quot;eval \\$($(brew --prefix)/bin/brew shellenv)&quot; &gt;&gt;~/.bash_profile echo &quot;eval \\$($(brew --prefix)/bin/brew shellenv)&quot; &gt;&gt;~/.profile # debian/ubuntu dependencies sudo apt-get install build-essential curl file git LinuxBrew golang installation brew install go The GO PATH should then be checked; it should typically exist at ~/go but it can be checked like this echo $GOPATH 11.2 Updating brew install dep brew upgrade dep brew cask install spotify 11.3 Sample Program Create a file called hello.go package main import &quot;fmt&quot; func main() { fmt.Printf(&quot;hello, world\\n&quot;) } The program can just be run with go run main.go Then compile the program go build Importing multiple things can be done on one line separated by a semicolon import (&quot;fmt&quot;; &quot;math&quot;) 11.4 Type conversion Check the type of a variable import(&quot;fmt&quot;;&quot;reflect&quot;) i := 5 fmt.Println(reflect.TypeOf(i)) Convert int to float64 var i int = 5 j := float64(i) Convert float64 to in x := 4.0 int(x) int to string s := strconv.Itoa(97) // s == &quot;97&quot; int64 to string var n int64 = 97 s := strconv.FormatInt(n, 10) // s == &quot;97&quot; (decimal) in base 10 string to int s := &quot;97&quot; if n, err := strconv.Atoi(s); err == nil { fmt.Println(n+1) } else { fmt.Println(s, &quot;is not an integer.&quot;) } string to int64 s := &quot;97&quot; n, err := strconv.ParseInt(s, 10, 64) if err == nil { fmt.Printf(&quot;%d of type %T&quot;, n, n) } int to int64 var n int = 97 m := int64(n) // safe 11.5 Strings Numbers can be converted to strings using strconv s := strconv.FormatFloat(3.1415, &#39;E&#39;, -1, 64) s := strconv.FormatInt(-42, 16) Strings can be assigned with or &quot; &quot;, but only the double quotes can use escape characters like newlines or tabs fmt.Println(&quot;Hello World\\n&quot;) Indexing returns bytes rather than strings so they must be converted if you want a string back string(&quot;Hello World&quot;[1]) Test if a string is a substring of another import &quot;strings&quot; strings.Contains(&quot;something&quot;,&quot;some&quot;) // true 11.6 Boolean Functions &amp;&amp;, ||, !, true, false 11.7 Variables Variables are statically typed and therefore must be declared when assigned var x string = &quot;Hello World&quot; Type declaration can be offloaded to the compiler using the following notation, and the compiler will try to infer the correct type var x = &quot;Hello World&quot; x := &quot;Hello World&quot; Constants are similar to variables, but their values cannot be reassigned const x string = &quot;Hello&quot; Multiple variables can also be declared at once, where each variable must occupy its own line var ( a = 5 b = 10 c = 15 ) Substrings s := &quot;something&quot; fmt.Println(s[:len(s)-5]) fmt.Println(s[2:6]) 11.8 Input User input var input float64 fmt.Scanf(&quot;%f&quot;, &amp;input) 11.9 Control Structures For loops can be written like the following i := 1 for i &lt;= 10 { fmt.Println(i) i += 1 } The variables a function will return can be defined at the beginning of the functions and then implicitely returned. func something(x int) (product int) { product = x * x return } Probably the easier for loop is like this one for i := 1; i &lt;= 10; i++ { fmt.Println(i) } A nice for loop to iterate through a range for i := range str1 { if str1[i] == str2[i] { count++ } } return count For loops can be used to iterate through slices too for _, num := range nums { If loops look gross, but it is required that the else statement is placed where it is shown here if true { } else if false { } Switches are also a thing switch input { case 1: fmt.Println(&quot;You entered one&quot;) case 2: fmt.Println(&quot;You entered two&quot;) case 3: fmt.Println(&quot;You entered three&quot;) } 11.10 Data Structures A blank array var x []int An array with five elements var x [5]int x[0] = 50 // the first element of the array equals 50 An easier way to create an array and can be multiline broken by the commas x := [5]float64{ 98, 93, 77, 82, 83 } Slices can have variable lengths and are typically associated with an array of fixed length. The following slice is 5 elements long, and is a segment of a 10 element-long array x := make([]float64, 5, 10) In a way that seems more similar to python, slicing an array can be done like this arr := [5]float64{1,2,3,4,5} x := arr[0:2] Adding data to a slice slice1 := []int{1,2,3} slice2 := append(slice1,4,5) Or multiple values can be added at once s = append(s, 2, 3, 4) A map is an unordered collection of key-value pairs (also known as a dictionary). A map is defined by assigning it to a variable and then defining the key type in brackets and the value type after the brackets var x make(map[string]int) x[&quot;key&quot;] = 10 fmt.Println(x) Creating a map with multiple items simultaneously elements := map[string]string{ &quot;H&quot;: &quot;Hydrogen&quot;, &quot;He&quot;: &quot;Helium&quot;, &quot;Li&quot;: &quot;Lithium&quot;, } Items can be deleted from a map using the delete function delete(x, &quot;key&quot;) Go provides functionality that checks whether a key lookup from a map was successful or not m,n := x[&quot;unknown&quot;] // this key does not exist fmt.Println(m,n) // m will equal 0 and n will equal false This check can be used in an if loop to only run a chunk of code if a key exists within a map. In the below code el equals the value and ok is true or false, if the key is found, ok equals true and the print statement is run if el, ok := elements[&quot;Li&quot;]; ok { fmt.Println(el[&quot;name&quot;], el[&quot;state&quot;]) } Iterate through a map for k, v := range kmers { fmt.Printf(&quot;key: %s value: %d\\n&quot;, k, v) } 11.11 Functions Below is a basic function that computes the mean of a map func average(xs []float64) (float64) { total := 0.0 for _, v := range xs { total += v } return total / float64(len(xs)) } A function can also take multiple different types of variables func lots_of_stuff(a int, b map[string]int64, c float64) (string, string, int64) { // do stuff } If it is desirable that a function takes maps of variable lengths a function can be designed like the one below func add(args ...int) int { total := 0 for _, v := range args { total += v } return total } Functions can also be placed within other functions like this func main() { add := func(x, y int) int { return x + y } fmt.Println(add(1,1)) } Closure refers to functions that utilize non-local variables func main() { x := 0 increment := func() int { x++ return x } fmt.Println(increment()) fmt.Println(increment()) } Recursion uses the same function recursively // a recursive function func factorial(x uint) uint { if x == 0 { return 1 } return x * factorial(x-1) } Deferring essentially moves a function call to the end of a function, like the following which closes the file after it is used f, _ := os.Open(filename) defer f.Close() 11.12 While loops These are not actually included in golang as in other languages but instead utilize for loops. This is a repeat-until loop: for { work() if condition { break } } or for ok := true; ok; ok = !condition { work() } A do-while loop for { work() if !condition { break } } or for ok := true; ok; ok = condition { work() } 11.13 Timing a function start := time.Now() t := time.Now() elapsed := t.Sub(start) fmt.Println(elapsed) 11.14 Pointers Pointers can be used to access the memory location of a variable and alter the value stored in that location func zero(xPtr *int) { *xPtr = 0 // asterisk signifies a pointer } func main() { x := 5 zero(&amp;x) // &amp; finds the address of a variable fmt.Println(x) // using a pointer allows the value to be changed } There is a built in function called new that takes a type as an argument, and allocates sufficient memory to hold that type and returns a pointer to it, and unlike other languages, because go is a garbage collected language, it will delete anything created by new when nothing refers to it anymore func one(xPtr *int) { *xPtr = 1 } func main() { xPtr := new(int) one(xPtr) fmt.Println(*xPtr) } 11.15 Structures These seem similar to classes and allow a new ‘type’ to be created type Circle struct { x float64 y float64 r float64 // values of the same type can be combined like this x, y, r float64 } A new structure can be created just like a typical variable var c Circle A structure can be created like pointers which will set all values to their zero value like 0, 0.0, &quot;&quot; and return a pointer c := new(Circle) Values for a structure can also be defined at variable creation time c := Circle{x: 0, y: 0, r: 5} // this is possible if you remember the order vars were defined c := Circle{0, 0, 5} Structure fields are accessed like class methods fmt.Println(c.x, c.y, c.r) c.x = 10 c.y = 5 When passing a structure to another function, its type is the name of the structure func circleArea(c Circle) float64 { return math.Pi * c.r * c.r } Fields of a structure that has been defined are not altered unless a pointer is used func circleArea(c *Circle) float64 { c.r = 10 return math.Pi * c.r * c.r } func main() { c:= Circle{0, 0, 5} fmt.Println(circleArea(&amp;c), c) } 11.16 Methods These can be added to structures so they can be directly accessed by automatically passing a pointer to the method. The area() receiver can be used for other structures, and does not have to be a unique word type Circle struct { x, y, r float64 } func (c *Circle) area() float64 { return math.Pi * c.r * c.r } func main() { c:= Circle{0, 0, 5} fmt.Println(c.area()) } Embedded Types is sort of like inheritance and gives a method access to all of the features of another structure. Here is a person structure type Person struct { Name string } func (p *Person) Talk() { fmt.Println(&quot;Hi, my name is&quot;, p.Name) } Here Android is defined to have the same properties as the person structure type Android struct { Person Model string } a := new(Android) a.Talk() 11.17 Packaging Building code into a package is a convenient way of then accessing the same methods without having to rebuild them. To create a package, make a folder with the same name as the package so math in this case. Then refer to the package name in a file within this folder like this: package math func Average(xs []float64) float64 { total := float64(0) for _, x := range xs { total += x } return total / float64(len(xs)) } Then save this file and build it from the same directory using: go install Finally, use this package by referring to its directory like this: package main import &quot;fmt&quot; // import &quot;chapter11/math&quot; //this format can be used if put in ~/go/src/ import &quot;./math&quot; func main() { xs := []float64{1,2,3,4} avg := math.Average(xs) fmt.Println(avg) } 11.18 Read/Write Files Here is how a file is opened f, err := os.Open(&quot;./data/genomes/schisto_small.fa&quot;) check(err) f.Close() // when done func check(e error) { if e != nil { panic(e) } } In Go 2.0 the try function can be used to open files a bit more elegantly. Instead of this: f, err := os.Open(filename) if err != nil { return …, err } Opening is simplified to this: f := try(os.Open(filename)) Read by a certain number of bytes at a time b1 := make([]byte, 61) n1, err := f.Read(b1) check(err) fmt.Printf(&quot;%d bytes: %s\\n&quot;, n1, string(b1)) Peek might be more efficient with many small read calls but reads next n bytes without advancing the reader r4 := bufio.NewReader(f) b4, err := r4.Peek(61) check(err) fmt.Printf(&quot;5 bytes: %s\\n&quot;, string(b4)) Scanners are useful ways to read newline delimited files scanner := bufio.NewScanner(f) for scanner.Scan() { fmt.Println(scanner.Text()) // Println retains the \\n } if err := scanner.Err(); err != nil { fmt.Fprintln(os.Stderr, &quot;reading standard input:&quot;, err) } Read from gzip file import(&quot;compress/gzip&quot;) file, err := os.Open(&quot;./data/genomes/schisto_small_2.fa.gz&quot;) f, err := gzip.NewReader(file) 11.19 Math Absolute value is pretty easy if using a float64 import(&quot;math&quot;) x := -4.0 math.Abs(x) Now, it seems to be super annoying to calculate the absolute value of an int. import(&quot;math&quot;) x := -5 int(math.Abs(float64(x))) 11.20 Concurrency 11.20.1 Goroutines Goroutines are lightweight threads that are managed by the Go runtime, and are run concurrently in the same address space as other function calls. package main import ( &quot;fmt&quot; &quot;time&quot; ) func say(s string) { for i := 0; i &lt; 5; i++ { time.Sleep(100 * time.Millisecond) fmt.Println(s) } } func main() { go say(&quot;world&quot;) say(&quot;hello&quot;) } 11.20.2 Channels Channels are a typed conduit through which values can be sent and received. They function with the &lt;- operator. ch &lt;- v // Send v to channel ch. v := &lt;-ch // Receive from ch, and // assign value to v. The default function of a channel is to block communication until both sides are ready. In the example below, the sum of the first and last three values in an array are calculated separately and then added together when both goroutines have completed. package main import &quot;fmt&quot; func sum(s []int, c chan int) { sum := 0 for _, v := range s { sum += v } c &lt;- sum // send sum to c } func main() { s := []int{7, 2, 8, -9, 4, 0} c := make(chan int) go sum(s[:len(s)/2], c) go sum(s[len(s)/2:], c) x, y := &lt;-c, &lt;-c // receive from c fmt.Println(x, y, x+y) Unlike typical channels, buffered channels only block receiving communication when the number of slots are full. In the below example, the buffered channel has room for two values, receives those two values and then prints them in order. package main import &quot;fmt&quot; func main() { ch := make(chan int, 2) ch &lt;- 1 ch &lt;- 20000 x := &lt;- ch fmt.Println(x) fmt.Println(&lt;-ch) } "],
["resources.html", "Chapter 12 Bioinformatics Resources 12.1 Cancer Datasets 12.2 Alzheimer’s 12.3 Genome Resources 12.4 Genomic Datasets 12.5 Computing Tools 12.6 Microbiome Datasets 12.7 eQTL/RNASeq and other Tools 12.8 Cell Biology 12.9 Other Data Resources", " Chapter 12 Bioinformatics Resources 12.1 Cancer Datasets CancerMine is a site that mines publication data to create a database of mutations labeled as drivers, oncogenes, or tumor suppressors. These classifications may help to understand the evolution of different cancers. The Cancer Cell Line Encyclopedia has a wealth of information from a large number of cancer cell lines. Project score has a number of genetic screens that may be useful in identifying pathways that are critical to cancer growth and survival cbioportal has information about codon changes in cancer but does not seem to have any sequence data dbGAP has info on genotypes and phenotypes, whatever the fuck that means Here is the page with instructions on how to get dbGAP access TCGA This page has the instructions on how to get TCGA access COSMIC Cosmic has a project called the Cancer Gene Census in which they are trying to catalog all mutations that have been implicated in playing a causal role in cancer They also have implemented convenient ways of directly downloading information and files from the database in python using the files hosted here. Mastermind from genomenon parsed all the articles on pubmed in order to find any and all information for each possible mutation GTEx has RNA-Seq, Exome Seq, WGseq, SNP arrays, gene expression arrays and more for cancer and non-cancer? TARGET is a dataset at the NIH that has childhood cancer data. 12.2 Alzheimer’s The Alzheimers sequencing project is gathering data to understand late onset alzheimer’s 12.3 Genome Resources Jax has a human to mouse gene matching list that provides gene location matching between human and mouse UCSC genome browser has an API for programmatic access 12.4 Genomic Datasets AllOfUs is sequencing and collecting other health data on a million individuals Color Genomics is one of three companies that will be doing the sequencing and testing for AllOfUs 1000 genomes 100,000 genomes TOPMed has a large number of samples spanning a number of different studies that include whole genome sequencing which can be helpful for understanding clonal dynamics in CHIP. 12.5 Computing Tools AWS looks like it has some healthcare and life sciences resources 12.6 Microbiome Datasets The Human Microbiome Project Data Portal from Michael Snyder’s group has longitudinal ’omics data that includes diseased and healthy timepoints. Michael Snyder’s iPOP Personal ’Omics Profiling has some interesting microbiome data specifically targeted to understanding diabetes. People respond differently to different drugs, and this appears to in part be due to the differential drug metabolism of their gut microbiome. Some of the differences can be observed when different strains of gut bacteria are isolated and directly exposed to drug to understand how the drugs are differentially metabolized (Zimmermann et al. 2019). The bacterial sequencing data is available here, and some of the extra drug screening data is available here. Multi-omics of Crohn’s and ulcerative colitis can be found as part of the Integrative Human Microbiome Project. 12.7 eQTL/RNASeq and other Tools Here are a good number of general tools to check out Here is a good python eQTL analysis genenetwork2 has a great deal of data and eQTL mapping tools The Eske Derks lab has some hosted data that walks through how to properly run a GWAS analysis (Marees et al. 2018). 12.8 Cell Biology HuBMAP is a group that is organizing tissues at the single-cell scale to map tissues and organisms spatially. 12.9 Other Data Resources The Earth Microbiome Project already has data available, and is trying to sequence all non-eukaryotic life on earth They have detailed information about the project in their PNAS paper from 2018 Information is Beautiful has some interesting datasets References "],
["references.html", "References", " References "]
]
